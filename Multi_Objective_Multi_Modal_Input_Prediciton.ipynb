{"cells":[{"cell_type":"markdown","source":["# Problem Formulation\n","\n","The main problem at hand is to conduct multi-objective prediction using multi-modal input. Other aspects of the assignment are designed to predict the price range or type of a house listing given either image or text based input. \n","\n","The input to the various models and problems consists of image and or text data. The image data consists of 7,627 RGB images, 6,101 for training and 1,526 for validation. These images are of the interiors of the house listing. The text data is given as a descriptions of the house from the owner of the listing. There is the same number of text inputs as there are image inputs with the same train validate split.\n","\n"," The output of the model if there is only one objective, is either type or price. Type, is the type of living situation the listing is, a house, apartment, loft, etc. each a representation of a number from 0 - 23.Price, is the price range associated with the listing, split into 3 categories, beginner, plus, premium, each being a representation of a number between 0 - 2. This a classification problem and the required data mining function.\n","\n","The main challenge presented by this problem is properly dealing with the different modalities of the inputs. Each modality must be individually processed and correctly integrated to produce accurate results. The creation of the multi-objective model architecture will be a challenge as this is new territory.  \n","\n","The impact of this problem is to help better understand the price range and type of living situation associated with an Airbnb listing.The company could use this information to give clients recommendations as to how much they should list their property. This could help make the client and the company more money.\n","\n","The ideal solution to this problem is a model that can correctly predict both the type and price of an Airbnb listing with high accuracy while maintaining a high degree of generalizability. The model should be able to take multi-modal input and produce multi-objective prediction. "],"metadata":{"id":"6B1NhFaF9PsM"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"KiMrkOjeWj1A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668628929687,"user_tz":300,"elapsed":225884,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"05bcd151-fbb7-463d-e415-5e8635433cbe"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-11-16 19:58:23--  https://github.com/CISC-873/Information-2021/releases/download/data/a4-5.zip\n","Resolving github.com (github.com)... 20.205.243.166\n","Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/406495726/4d095bba-8b9b-4be4-8738-83f8ff5b0d18?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221116T195823Z&X-Amz-Expires=300&X-Amz-Signature=5885d99c9c7ae10ec48d55bd04b932b104ea61a507a03f92dcfcd9604218ba39&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=406495726&response-content-disposition=attachment%3B%20filename%3Da4-5.zip&response-content-type=application%2Foctet-stream [following]\n","--2022-11-16 19:58:24--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/406495726/4d095bba-8b9b-4be4-8738-83f8ff5b0d18?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221116%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221116T195823Z&X-Amz-Expires=300&X-Amz-Signature=5885d99c9c7ae10ec48d55bd04b932b104ea61a507a03f92dcfcd9604218ba39&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=406495726&response-content-disposition=attachment%3B%20filename%3Da4-5.zip&response-content-type=application%2Foctet-stream\n","Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 639078419 (609M) [application/octet-stream]\n","Saving to: ‘a4-5.zip’\n","\n","a4-5.zip            100%[===================>] 609.47M  1.91MB/s    in 3m 36s  \n","\n","2022-11-16 20:02:01 (2.82 MB/s) - ‘a4-5.zip’ saved [639078419/639078419]\n","\n"]}],"source":["# you can also download the data by running the following line (linux only) \n","# if you already got the data from kaggle, you can skip this cell.\n","\n","! wget https://github.com/CISC-873/Information-2021/releases/download/data/a4-5.zip\n","\n","! unzip -q a4-5.zip"]},{"cell_type":"code","source":["import os\n","\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","from PIL import Image\n","import pandas as pd\n","import os\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from keras.optimizers import Adam\n","\n","from keras.preprocessing import sequence\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, MaxPool2D, Input, Attention, Dropout, Embedding, LSTM, Bidirectional, InputLayer, Conv2D, MaxPooling2D, Flatten\n","from keras.datasets import imdb \n","from keras_preprocessing.sequence import pad_sequences\n","\n","from sklearn.model_selection import train_test_split\n","\n","import matplotlib.pyplot as plt\n","\n","from keras.preprocessing.text import Tokenizer\n","from sklearn.model_selection import train_test_split\n","from pprint import pprint\n","\n","import matplotlib.pyplot as plt\n","from keras.applications import MobileNet\n","\n","from keras.layers import Layer\n","import keras.backend as K\n","\n","import statistics\n","from statistics import mean\n"],"metadata":{"id":"m1iVEKle1Gk0","executionInfo":{"status":"ok","timestamp":1668629085394,"user_tz":300,"elapsed":313,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"BEur8cLCR1Zk","executionInfo":{"status":"ok","timestamp":1668628940172,"user_tz":300,"elapsed":378,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"outputs":[],"source":["xy_train_df = pd.read_csv('train_xy.csv')\n","x_test_df = pd.read_csv('test_x.csv')"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["95f807319dad4c5d8a735ff28c2aa33f","0432bf5b52cb449e97a15eab0069264b","34f87b3cf05345a9ae3ad45400d2b4f8","ee2d6274b97e40a186fe1f1c2a63f188","1078c1420028485f8b216b2d6a802097","e5f2a7c72a824b4d9788ae62265693b7","7e917e0737234a268bfb900790c923b7","58825daa17cf4c56bbe32f83b245d4fd","35daeee4dc744cf4991a06346d55e2d1","331cdebf857a4df8a209a9643a9345c9","454400d769b941189c6c8ec4af9141fb"]},"id":"7NGojpH_R1Zl","outputId":"c8b33d13-9bbf-4c0b-f180-fb6d1d2e5eef","executionInfo":{"status":"ok","timestamp":1668629001352,"user_tz":300,"elapsed":59801,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/7627 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95f807319dad4c5d8a735ff28c2aa33f"}},"metadata":{}}],"source":["# preprocess image data\n","\n","def load_image(file):\n","    try:\n","        image = Image.open(\n","            file\n","        ).convert('RGB').resize((32, 32))\n","        arr = np.array(image)\n","    except:\n","        arr = np.zeros((32, 32, 3))\n","    return arr\n","\n","\n","# loading images:\n","x_image = np.array([load_image(i) for i in tqdm(xy_train_df.image)])\n","\n","# loading summary: (force convert some of the non-string cell to string)\n","x_text = xy_train_df.summary.astype('str')\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":283},"id":"Ayqfe7rQS0Mk","outputId":"4e954899-f2ed-41d3-b0fe-bdcdf62e9de1","executionInfo":{"status":"ok","timestamp":1668629037752,"user_tz":300,"elapsed":10,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7fb85c666390>"]},"metadata":{},"execution_count":5},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["# check image loading\n","plt.imshow(x_image[0, :, :, 0])"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1z3FEfYsR1Zm","outputId":"2af247ee-a1b1-4c21-cb9b-11cfdc2edf7b","executionInfo":{"status":"ok","timestamp":1668629037753,"user_tz":300,"elapsed":8,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["unique values for price category 3 [1 0 2]\n","unique values for type category 24 [ 1 17 22 10 18 20  5  2  8  4 23 13 15 16 14 11 19  0 21  3  6 12  7  9]\n","(6101, 32, 32, 3)\n","(1526, 32, 32, 3)\n","(6101,)\n","(1526,)\n","(6101,)\n","(1526,)\n"]}],"source":["# labels:\n","y_price = xy_train_df.price\n","y_type = xy_train_df.type.astype('category').cat.codes\n","\n","len_price = len(y_price.unique())\n","len_type = len(y_type.unique())\n","print('unique values for price category', len_price, y_price.unique())\n","print('unique values for type category', len_type, y_type.unique())\n","\n","# splitting:\n","\n","x_tr_image, x_vl_image, x_tr_text, x_vl_text, y_tr_price, y_vl_price, y_tr_type, y_vl_type = train_test_split(\n","    x_image, \n","    x_text,\n","    y_price,\n","    y_type,\n","    test_size=0.2)\n","\n","print(np.shape(x_tr_image))\n","print(np.shape(x_vl_image))\n","print(np.shape(y_tr_type))\n","print(np.shape(y_vl_type))\n","print(np.shape(y_tr_price))\n","print(np.shape(y_vl_price))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xh70MnCDR1Zm","outputId":"1ec57fcd-86df-4b19-a178-61241aac58d9","executionInfo":{"status":"ok","timestamp":1668629090616,"user_tz":300,"elapsed":720,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["(6101, 100)\n","(1526, 100)\n"]}],"source":["# preprocess text data\n","\n","vocab_size = 40000\n","max_len = 100\n","\n","\n","# build vocabulary from training set\n","tokenizer = Tokenizer(num_words=vocab_size)\n","tokenizer.fit_on_texts(x_tr_text)\n","\n","\n","def _preprocess(list_of_text):\n","    return pad_sequences(\n","        tokenizer.texts_to_sequences(list_of_text),\n","        maxlen=max_len,\n","        padding='post',\n","    )\n","    \n","\n","# padding is done inside: \n","x_tr_text_id = _preprocess(x_tr_text)\n","x_vl_text_id = _preprocess(x_vl_text)\n","\n","print(x_tr_text_id.shape)\n","print(x_vl_text_id.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p5E5zYSiR1Zm","outputId":"5ea10a61-9ce0-4aee-cae3-2691de4de45f","executionInfo":{"status":"ok","timestamp":1668629094598,"user_tz":300,"elapsed":2,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['spacious apartment in the heart of the plateau mont royal perfect location '\n"," 'for a cultural experience within a 10 minute walking distance you will find '\n"," 'cafés restaurants fromageries grocery store laurier park bars etc near '\n"," 'public transportation bixi bus metro',\n"," 'dans rosemont beau logement 4 pièces avec une chambre fermée grand salon '\n"," 'cuisine et salle de bain accès à la cour arrière avec table chaises de patio '\n"," 'et bbq cuisine toute équipée et laveuse sécheuse sur place wi fi et '\n"," 'télévision satellite dans quartier résidentiel avec tous les services à '\n"," 'proximité épicerie pharmacie dépanneur parc beaubien et parc molson à '\n"," \"distance de marche plusieurs lignes d'autobus à proximité ainsi que la \"\n"," 'station de métro st michel à 8 minutes de marche',\n"," 'grand appartement mile end près des avenues laurier du parc et mont royal à '\n"," '10 minutes à pied du mont royal une chambre avec lit double une chambre avec '\n"," 'lit simple un bureau avec canapé lit salon cuisine récemment rénové très '\n"," 'lumineux',\n"," 'cet appartement dispose de deux chambres séparées avec un lit queen size et '\n"," 'un lit double il est idéalement situé dans l’un des quartiers les plus '\n"," \"populaires de montréal avec toutes les commodités et l'ambiance montréalaise \"\n"," 'this apartment has two separate bedrooms with a queen size bed and a double '\n"," \"bed it is ideally located in one of montreal's most popular neighborhoods \"\n"," 'you will benefit from all the amenities and a lively montreal neighborhood '\n"," 'life at a corner of the apartment',\n"," 'maison avec abondante luminosité située sur une rue très tranquille à '\n"," 'proximité des transports en commun épicerie boulangerie fruiterie '\n"," 'poissonnerie charcuterie la banlieue en ville']\n"]}],"source":["pprint(tokenizer.sequences_to_texts(x_tr_text_id[:5]))"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"75tmwBWAR1Zm","outputId":"a2cee9e1-fd36-4d3c-f3a7-8985ec33985e","executionInfo":{"status":"ok","timestamp":1668629094899,"user_tz":300,"elapsed":1,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["total words in the dictionary: 40000\n"]}],"source":["print('total words in the dictionary:', tokenizer.num_words)"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["e20b568fd87f4894bc1959ef754205cf","78760a297e6a42c196145d9bdb7297e8","b4a12b96c3d8411d9774376e9c9e9c5e","1bc753cbe8c64121ab7f19443f5178f8","18deb17ab1de42c0a265b376830c6e42","a069732fcd774c9096e0f7bf0215e531","264e4635ad0b4136a859d8488f276962","4659dd22265841b3a852e93dbc590430","a0824af7993d4f548379ce9d63f61e86","30d11a9cfff043c5ac3b2a5fb6d08982","b2be3c79a45049a796c91dc32f415c8e"]},"id":"6fxoXMDER1Zm","outputId":"37914852-3283-44d8-e7b1-488b71d61420","executionInfo":{"status":"ok","timestamp":1668630760988,"user_tz":300,"elapsed":57979,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/7360 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e20b568fd87f4894bc1959ef754205cf"}},"metadata":{}}],"source":["x_test_summary = _preprocess(x_test_df.summary.astype(str))\n","x_test_image = np.array([load_image(i) for i in tqdm(x_test_df.image)])\n"]},{"cell_type":"markdown","source":["# Trial 1\n"," \n","reason for change?\n","\n","The first trial of the assignment will tackle the text input given in the assignment. The best models for text input are LSTMs, in this trail I will use a bidirectional LSTM which is more powerful than a standard LSTM. They can use information in both directions of the sequence, both from the start and end of the sequence. This allows for stronger modelling of dependencies between words, this can be utilized to better classify the listing descriptions given as input to the model in this trial. \n","\n","expected outcome?\n","\n","In this trial the model will take in text descriptions of the listings to classify the price range of the listing. The power of the bidirectional LSTM should be able to accurately predict the price range of the listing, to above 50% accuracy. With larger architectures the model should be able to perform at a higher accuracy."],"metadata":{"id":"J1kW_KCn72nL"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"eHUXFXX5R1Zm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668629103184,"user_tz":300,"elapsed":2805,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"53b8bdf7-95db-4aa5-a483-75f09f0c8383"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 100, 16)           640000    \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 128)              41472     \n"," l)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," dense (Dense)               (None, 3)                 387       \n","                                                                 \n","=================================================================\n","Total params: 681,859\n","Trainable params: 681,859\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# variables defined to help construct the model\n","num_words = 40000 # tokenizer.num_words\n","maxlen = max_len\n","batch_size = 16\n","\n","model_text = Sequential() \n","model_text.add(Embedding(num_words, batch_size, input_length=maxlen)) # represent words with dense vector representations\n","model_text.add(Bidirectional(LSTM(64))) # add into the model a bidirectional LSTM layer\n","model_text.add(Dropout(0.5)) # add in drop out regularization to help prevent overfitting\n","model_text.add(Dense(3, activation='softmax',)) # final output layer matching the 3 class options for listing price\n","model_text.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['SparseCategoricalAccuracy']) \n","\n","model_text.summary()"]},{"cell_type":"code","source":["# Train the model with the required trianing and validation sets\n","history = model_text.fit(x_tr_text_id, y_tr_price,\n","           batch_size=batch_size,\n","           epochs=20,\n","           validation_data=[x_vl_text_id, y_vl_price])\n","# fit model and print the mean of both train and validation accuracies \n","print(mean((history.history['val_sparse_categorical_accuracy'])))\n","print(mean((history.history['sparse_categorical_accuracy'])))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pWP2XmgBH2Xi","executionInfo":{"status":"ok","timestamp":1668629256681,"user_tz":300,"elapsed":103970,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"1c97119e-1ff1-465a-9a92-f45a2fdf043e"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","382/382 [==============================] - 12s 14ms/step - loss: 0.8290 - sparse_categorical_accuracy: 0.6292 - val_loss: 0.7442 - val_sparse_categorical_accuracy: 0.6822\n","Epoch 2/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.6902 - sparse_categorical_accuracy: 0.6917 - val_loss: 0.7147 - val_sparse_categorical_accuracy: 0.6900\n","Epoch 3/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.5993 - sparse_categorical_accuracy: 0.7495 - val_loss: 0.7334 - val_sparse_categorical_accuracy: 0.6763\n","Epoch 4/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.5351 - sparse_categorical_accuracy: 0.7823 - val_loss: 0.8079 - val_sparse_categorical_accuracy: 0.6291\n","Epoch 5/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.4645 - sparse_categorical_accuracy: 0.8187 - val_loss: 0.8689 - val_sparse_categorical_accuracy: 0.6704\n","Epoch 6/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.3993 - sparse_categorical_accuracy: 0.8469 - val_loss: 0.9730 - val_sparse_categorical_accuracy: 0.6710\n","Epoch 7/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.3415 - sparse_categorical_accuracy: 0.8733 - val_loss: 1.0927 - val_sparse_categorical_accuracy: 0.6304\n","Epoch 8/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.3069 - sparse_categorical_accuracy: 0.8894 - val_loss: 1.1153 - val_sparse_categorical_accuracy: 0.6284\n","Epoch 9/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.2679 - sparse_categorical_accuracy: 0.9003 - val_loss: 1.2266 - val_sparse_categorical_accuracy: 0.6212\n","Epoch 10/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.2371 - sparse_categorical_accuracy: 0.9153 - val_loss: 1.2580 - val_sparse_categorical_accuracy: 0.6448\n","Epoch 11/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.2044 - sparse_categorical_accuracy: 0.9225 - val_loss: 1.2899 - val_sparse_categorical_accuracy: 0.6448\n","Epoch 12/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.1968 - sparse_categorical_accuracy: 0.9239 - val_loss: 1.4584 - val_sparse_categorical_accuracy: 0.6337\n","Epoch 13/20\n","382/382 [==============================] - 6s 17ms/step - loss: 0.1913 - sparse_categorical_accuracy: 0.9308 - val_loss: 1.5935 - val_sparse_categorical_accuracy: 0.6311\n","Epoch 14/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.1638 - sparse_categorical_accuracy: 0.9384 - val_loss: 1.5330 - val_sparse_categorical_accuracy: 0.6245\n","Epoch 15/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.1526 - sparse_categorical_accuracy: 0.9451 - val_loss: 1.6398 - val_sparse_categorical_accuracy: 0.6239\n","Epoch 16/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.1548 - sparse_categorical_accuracy: 0.9400 - val_loss: 1.6181 - val_sparse_categorical_accuracy: 0.6343\n","Epoch 17/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.1524 - sparse_categorical_accuracy: 0.9418 - val_loss: 1.7487 - val_sparse_categorical_accuracy: 0.6337\n","Epoch 18/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.1348 - sparse_categorical_accuracy: 0.9485 - val_loss: 1.8997 - val_sparse_categorical_accuracy: 0.6278\n","Epoch 19/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.1396 - sparse_categorical_accuracy: 0.9475 - val_loss: 1.6345 - val_sparse_categorical_accuracy: 0.6291\n","Epoch 20/20\n","382/382 [==============================] - 5s 12ms/step - loss: 0.1270 - sparse_categorical_accuracy: 0.9531 - val_loss: 1.8544 - val_sparse_categorical_accuracy: 0.6153\n","0.6421035468578339\n","0.874422225356102\n"]}]},{"cell_type":"markdown","source":["###thoughts and observations for trial 1\n","\n","The bidirectional LSTM was able to perform with 64.21% validation accuracy and 62.71% test accuracy. There was presence of overfitting as the training accuracy approaches near perfect accuracy while the validation accuracy decreases over each iteration staying within 67%-62%. Providing a good starting point for the LSTM model. \n","\n","###plan for trial 2\n","\n","For trail two I will add attention to the architecture presented in trail 1. I will also try to increase the number of dense layers to try to further help with performance. Attention is a powerful tool that should help with the models performance. \n","\n"],"metadata":{"id":"no5l2a9K8AOC"}},{"cell_type":"markdown","source":["# Trial 2\n"," \n","reason for change?\n","\n","Attention is a mechanism which allows RNN/LSTM architectures to not only look at the last hidden state but all states of the encoder. Attention extracts information from all previous states of the sequence and assigns importance to certain elements of the input. This allows for the model to pay more attention to important elements of the sequence. This is especially helpful for long sequences. Although the input sequences in this assignment aren't that long attention is still a powerful tool that could help with performance. The added layer to the architecture may also help with accuracy, it is known that added depth helps with performance. \n","\n","expected outcome?\n","\n","Since the length of the sequences are not very long I suspect that adding attention will not provide much help to the model. I do think the added layer and with help from the attention mechanism could slightly improve the accuracy of the model. "],"metadata":{"id":"dKHULeMuX6-u"}},{"cell_type":"code","source":["#https://machinelearningmastery.com/adding-a-custom-attention-layer-to-recurrent-neural-network-in-keras/\n","# the attension mechanisum was taken from the the above article with an open source tutorial.\n","\n","class attention(Layer):\n","    def __init__(self,**kwargs):\n","        super(attention,self).__init__(**kwargs)\n","\n","    def build(self,input_shape):\n","        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n","        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n","        super(attention, self).build(input_shape)\n","\n","    def call(self,x):\n","        # supply scores of weights and bias from training phase to tanh activation function \n","        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n","        # remove the dimention\n","        at=K.softmax(et)\n","        # reshape the array to become proper format to be a tensor\n","        at=K.expand_dims(at,axis=-1)\n","        # ouput the context vecor\n","        output=x*at\n","        return K.sum(output,axis=1)\n","\n","    def compute_output_shape(self,input_shape):\n","        return (input_shape[0],input_shape[-1])\n","\n","    def get_config(self):\n","        return super(attention,self).get_config()"],"metadata":{"id":"lFaENCwUCdcP","executionInfo":{"status":"ok","timestamp":1668629467927,"user_tz":300,"elapsed":315,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["num_words = 40000 # tokenizer.num_words\n","maxlen = max_len\n","batch_size = 16\n","# using attention in the following way requires the tensorflow functional API\n","# The only added layer is the attention layer, all other layers were explained in previous cells\n","input_words = Input((maxlen, ))\n","x_words = (Embedding(num_words, batch_size, input_length=maxlen))(input_words)\n","x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n","x = attention()(x_words)\n","x = Dropout(0.5)(x)\n","x = Dense(50, activation='relu')(x)\n","x = Dropout(0.5)(x)\n","pred = Dense(3, activation='softmax')(x)\n","model_attention = keras.Model(inputs=input_words, outputs=pred)\n","\n","model_attention.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['SparseCategoricalAccuracy']) \n","model_attention.summary()\n"],"metadata":{"id":"yURGV6K5UdG7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668629470097,"user_tz":300,"elapsed":390,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"9ec7e4e6-7f35-434f-d263-fbe0f9439cde"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 100)]             0         \n","                                                                 \n"," embedding_1 (Embedding)     (None, 100, 16)           640000    \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 100, 100)         26800     \n"," nal)                                                            \n","                                                                 \n"," attention (attention)       (None, 100)               200       \n","                                                                 \n"," dropout_1 (Dropout)         (None, 100)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 50)                5050      \n","                                                                 \n"," dropout_2 (Dropout)         (None, 50)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 3)                 153       \n","                                                                 \n","=================================================================\n","Total params: 672,203\n","Trainable params: 672,203\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Train the model with the required trianing and validation sets\n","history = model_attention.fit(x_tr_text_id, y_tr_price,\n","           batch_size=batch_size,\n","           epochs=20,\n","           validation_data=[x_vl_text_id, y_vl_price])\n","# fit model and print the mean of both train and validation accuracies \n","print(\"mean val accuracy:\",mean((history.history['val_sparse_categorical_accuracy'])))\n","print(\"mean train accuracy:\",mean((history.history['sparse_categorical_accuracy'])))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rZCcFBC1DXry","executionInfo":{"status":"ok","timestamp":1668631179376,"user_tz":300,"elapsed":124669,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"2d32f1f9-5a6f-4459-c2b0-04eaf54894df"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","382/382 [==============================] - 5s 14ms/step - loss: 0.1313 - sparse_categorical_accuracy: 0.9495 - val_loss: 2.2491 - val_sparse_categorical_accuracy: 0.6147\n","Epoch 2/20\n","382/382 [==============================] - 5s 14ms/step - loss: 0.1173 - sparse_categorical_accuracy: 0.9523 - val_loss: 2.1783 - val_sparse_categorical_accuracy: 0.6206\n","Epoch 3/20\n","382/382 [==============================] - 7s 19ms/step - loss: 0.1107 - sparse_categorical_accuracy: 0.9525 - val_loss: 3.3252 - val_sparse_categorical_accuracy: 0.6153\n","Epoch 4/20\n","382/382 [==============================] - 8s 20ms/step - loss: 0.1164 - sparse_categorical_accuracy: 0.9539 - val_loss: 2.8641 - val_sparse_categorical_accuracy: 0.6166\n","Epoch 5/20\n","382/382 [==============================] - 5s 14ms/step - loss: 0.1189 - sparse_categorical_accuracy: 0.9535 - val_loss: 2.9081 - val_sparse_categorical_accuracy: 0.6265\n","Epoch 6/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1176 - sparse_categorical_accuracy: 0.9533 - val_loss: 2.8124 - val_sparse_categorical_accuracy: 0.6265\n","Epoch 7/20\n","382/382 [==============================] - 5s 14ms/step - loss: 0.1148 - sparse_categorical_accuracy: 0.9562 - val_loss: 2.8332 - val_sparse_categorical_accuracy: 0.6140\n","Epoch 8/20\n","382/382 [==============================] - 7s 18ms/step - loss: 0.1086 - sparse_categorical_accuracy: 0.9553 - val_loss: 3.1238 - val_sparse_categorical_accuracy: 0.6055\n","Epoch 9/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1081 - sparse_categorical_accuracy: 0.9536 - val_loss: 2.6767 - val_sparse_categorical_accuracy: 0.6212\n","Epoch 10/20\n","382/382 [==============================] - 5s 13ms/step - loss: 0.0991 - sparse_categorical_accuracy: 0.9566 - val_loss: 3.7053 - val_sparse_categorical_accuracy: 0.6330\n","Epoch 11/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.1067 - sparse_categorical_accuracy: 0.9544 - val_loss: 3.0026 - val_sparse_categorical_accuracy: 0.6298\n","Epoch 12/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.1032 - sparse_categorical_accuracy: 0.9580 - val_loss: 3.3379 - val_sparse_categorical_accuracy: 0.6265\n","Epoch 13/20\n","382/382 [==============================] - 7s 19ms/step - loss: 0.0919 - sparse_categorical_accuracy: 0.9615 - val_loss: 3.4391 - val_sparse_categorical_accuracy: 0.6239\n","Epoch 14/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.0883 - sparse_categorical_accuracy: 0.9608 - val_loss: 3.7899 - val_sparse_categorical_accuracy: 0.6304\n","Epoch 15/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1121 - sparse_categorical_accuracy: 0.9559 - val_loss: 1.7996 - val_sparse_categorical_accuracy: 0.6088\n","Epoch 16/20\n","382/382 [==============================] - 7s 19ms/step - loss: 0.1125 - sparse_categorical_accuracy: 0.9589 - val_loss: 2.7867 - val_sparse_categorical_accuracy: 0.6278\n","Epoch 17/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.0945 - sparse_categorical_accuracy: 0.9579 - val_loss: 3.5401 - val_sparse_categorical_accuracy: 0.6278\n","Epoch 18/20\n","382/382 [==============================] - 6s 17ms/step - loss: 0.0966 - sparse_categorical_accuracy: 0.9598 - val_loss: 3.8279 - val_sparse_categorical_accuracy: 0.6265\n","Epoch 19/20\n","382/382 [==============================] - 7s 18ms/step - loss: 0.0923 - sparse_categorical_accuracy: 0.9607 - val_loss: 3.6917 - val_sparse_categorical_accuracy: 0.6166\n","Epoch 20/20\n","382/382 [==============================] - 7s 18ms/step - loss: 0.0867 - sparse_categorical_accuracy: 0.9623 - val_loss: 3.9790 - val_sparse_categorical_accuracy: 0.6212\n","mean val accuracy: 0.6216579258441925\n","mean train accuracy: 0.9563432186841965\n"]}]},{"cell_type":"markdown","source":["###thoughts and observations for trial 2\n","\n","bi-directional LSTM\n","\n","*   mean validation accuracy - 64.21% \n","*   mean train accuracy       - 87.44% \n","*   test accuracy - 62.71%\n","\n","bi-directional LSTM in conjunction with attention \n","\n","*   mean validation accuracy - 64.1% \n","*   mean train accuracy       - 87.14% \n","*   test accuracy - 65.28%\n","\n","\n","\n","Adding attention improved the performance of the model in terms of test accuracy. The train and validation accuracies were unchanged, although the more important value, test accuracy, was improved. Although the input sequences are not very long attention still provided a noticeable affect. I suspect adding only one more dense layer did not improve the model and the main improvement came from the addition of attention. Overfitting is still present and will be addressed in the next trial.\n","\n","###plan for trial 3\n","\n","For trial 3 I am going to increase the number of dense layers even further and alter the drop out rate. This is an attempt to further decrease the presence of overfitting and further increase the accuracy of the model. \n"," \n"],"metadata":{"id":"LbsULcra_OcS"}},{"cell_type":"markdown","source":["# Trial 3\n"," \n","reason for change?\n","\n","It is widely known that increasing the depth of a neural network results in better performance in majority of cases. In this trial I will increase the number of dense layers from 2 to 6. The drop out rate is also increased from 0.5 to 0.7 to further help improve the overfitting present in the training process. Increasing the dropout rate will drop a higher percentage of nodes for each iteration, regularizing the model to a further extent. \n","\n","expected outcome?\n","\n","Increasing the number of layers by a significant amount should improve the accuracy of the model, and increasing the drop out rate should reduce overfitting. The combination of these two changes should increase the accuracy of the model even if it is a small amount. "],"metadata":{"id":"V3W0H-wf-_29"}},{"cell_type":"code","source":["num_words = 40000 # tokenizer.num_words\n","maxlen = max_len\n","batch_size = 16\n","# Using same functional arcitecture from previous cell\n","# adding more layers\n","input_words = Input((maxlen, ))\n","x_words = (Embedding(num_words, batch_size, input_length=maxlen))(input_words)\n","x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n","x = attention()(x_words) #Attention(maxlen)(x_words)\n","# added more fully connected layers \n","x = Dense(50, activation='relu')(x)\n","x = Dense(50, activation='relu')(x)\n","x = Dense(50, activation='relu')(x)\n","x = Dense(50, activation='relu')(x)\n","x = Dropout(0.7)(x) # increase drop out rate\n","x = Dense(50, activation='relu')(x)\n","x = Dropout(0.7)(x) # increase drop out rate\n","pred = Dense(3, activation='softmax')(x)\n","model_attention_drop = keras.Model(inputs=input_words, outputs=pred)\n","\n","model_attention_drop.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['SparseCategoricalAccuracy']) \n","model_attention_drop.summary()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35Y74iRfDpop","executionInfo":{"status":"ok","timestamp":1668630487398,"user_tz":300,"elapsed":788,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"1e2e7250-eddd-4d9a-c8e8-57dcade099f2"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, 100)]             0         \n","                                                                 \n"," embedding_2 (Embedding)     (None, 100, 16)           640000    \n","                                                                 \n"," bidirectional_2 (Bidirectio  (None, 100, 100)         26800     \n"," nal)                                                            \n","                                                                 \n"," attention_1 (attention)     (None, 100)               200       \n","                                                                 \n"," dense_3 (Dense)             (None, 50)                5050      \n","                                                                 \n"," dense_4 (Dense)             (None, 50)                2550      \n","                                                                 \n"," dense_5 (Dense)             (None, 50)                2550      \n","                                                                 \n"," dense_6 (Dense)             (None, 50)                2550      \n","                                                                 \n"," dense_7 (Dense)             (None, 50)                2550      \n","                                                                 \n"," dense_8 (Dense)             (None, 50)                2550      \n","                                                                 \n"," dense_9 (Dense)             (None, 50)                2550      \n","                                                                 \n"," dense_10 (Dense)            (None, 50)                2550      \n","                                                                 \n"," dropout_3 (Dropout)         (None, 50)                0         \n","                                                                 \n"," dense_11 (Dense)            (None, 50)                2550      \n","                                                                 \n"," dropout_4 (Dropout)         (None, 50)                0         \n","                                                                 \n"," dense_12 (Dense)            (None, 3)                 153       \n","                                                                 \n","=================================================================\n","Total params: 692,603\n","Trainable params: 692,603\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Train the model with the required trianing and validation sets\n","history = model_attention_drop.fit(x_tr_text_id, y_tr_price,\n","           batch_size=batch_size,\n","           epochs=20,\n","           validation_data=[x_vl_text_id, y_vl_price])\n","\n","# fit model and print the mean of both train and validation accuracies \n","print(\"mean val accuracy:\",mean((history.history['val_sparse_categorical_accuracy'])))\n","print(\"mean train accuracy:\",mean((history.history['sparse_categorical_accuracy'])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FoUKecYhKM8X","executionInfo":{"status":"ok","timestamp":1668631860344,"user_tz":300,"elapsed":114231,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"7bd6783d-8482-4a8a-a8d6-303d391c57ca"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.1864 - sparse_categorical_accuracy: 0.9110 - val_loss: 23.1150 - val_sparse_categorical_accuracy: 0.5793\n","Epoch 2/20\n","382/382 [==============================] - 6s 14ms/step - loss: 0.1759 - sparse_categorical_accuracy: 0.9169 - val_loss: 25.6872 - val_sparse_categorical_accuracy: 0.5799\n","Epoch 3/20\n","382/382 [==============================] - 7s 17ms/step - loss: 0.1932 - sparse_categorical_accuracy: 0.9141 - val_loss: 16.9019 - val_sparse_categorical_accuracy: 0.5780\n","Epoch 4/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.1654 - sparse_categorical_accuracy: 0.9197 - val_loss: 27.4386 - val_sparse_categorical_accuracy: 0.5767\n","Epoch 5/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1942 - sparse_categorical_accuracy: 0.9177 - val_loss: 27.1611 - val_sparse_categorical_accuracy: 0.5773\n","Epoch 6/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1764 - sparse_categorical_accuracy: 0.9192 - val_loss: 22.0267 - val_sparse_categorical_accuracy: 0.5832\n","Epoch 7/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1635 - sparse_categorical_accuracy: 0.9225 - val_loss: 22.7579 - val_sparse_categorical_accuracy: 0.5924\n","Epoch 8/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1551 - sparse_categorical_accuracy: 0.9261 - val_loss: 33.3304 - val_sparse_categorical_accuracy: 0.5793\n","Epoch 9/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1842 - sparse_categorical_accuracy: 0.9125 - val_loss: 29.1988 - val_sparse_categorical_accuracy: 0.5852\n","Epoch 10/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1763 - sparse_categorical_accuracy: 0.9158 - val_loss: 23.9936 - val_sparse_categorical_accuracy: 0.5799\n","Epoch 11/20\n","382/382 [==============================] - 6s 14ms/step - loss: 0.1938 - sparse_categorical_accuracy: 0.9135 - val_loss: 14.2918 - val_sparse_categorical_accuracy: 0.5891\n","Epoch 12/20\n","382/382 [==============================] - 6s 14ms/step - loss: 0.1737 - sparse_categorical_accuracy: 0.9236 - val_loss: 10.3232 - val_sparse_categorical_accuracy: 0.5668\n","Epoch 13/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1504 - sparse_categorical_accuracy: 0.9238 - val_loss: 23.6905 - val_sparse_categorical_accuracy: 0.5839\n","Epoch 14/20\n","382/382 [==============================] - 5s 14ms/step - loss: 0.1431 - sparse_categorical_accuracy: 0.9333 - val_loss: 22.1003 - val_sparse_categorical_accuracy: 0.5891\n","Epoch 15/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.1817 - sparse_categorical_accuracy: 0.9159 - val_loss: 25.2741 - val_sparse_categorical_accuracy: 0.5688\n","Epoch 16/20\n","382/382 [==============================] - 6s 14ms/step - loss: 0.1506 - sparse_categorical_accuracy: 0.9300 - val_loss: 35.6791 - val_sparse_categorical_accuracy: 0.5970\n","Epoch 17/20\n","382/382 [==============================] - 6s 14ms/step - loss: 0.1527 - sparse_categorical_accuracy: 0.9297 - val_loss: 28.1546 - val_sparse_categorical_accuracy: 0.5734\n","Epoch 18/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1809 - sparse_categorical_accuracy: 0.9236 - val_loss: 16.8742 - val_sparse_categorical_accuracy: 0.5727\n","Epoch 19/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1470 - sparse_categorical_accuracy: 0.9343 - val_loss: 39.1570 - val_sparse_categorical_accuracy: 0.5878\n","Epoch 20/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.1632 - sparse_categorical_accuracy: 0.9272 - val_loss: 22.9539 - val_sparse_categorical_accuracy: 0.5708\n","mean val accuracy: 0.5805373519659043\n","mean train accuracy: 0.9215128690004348\n"]}]},{"cell_type":"markdown","source":["###thoughts and observations for trial 3\n","\n","Trial 2\n","\n","*   mean validation accuracy - 64.1% \n","*   mean train accuracy       - 87.14% \n","*   test accuracy - 65.28%\n","\n","Trial 3\n","\n","*   mean validation accuracy - 58.05% \n","*   mean train accuracy       - 92.15% \n","*   test accuracy - 65.35%\n","\n","The results are very unexpected, the added layers and increased dropout rate actually made the overfitting problem worse and in turn resulted in a worse validation accuracy. You would think the test accuracy would also decrease, although it produced a slightly higher test accuracy than the validation accuracy. I can not confidently explain why this is happening, although further tuning into the optimal number of layers and drop out rate could be conducted to get an optimal set of hyper parameters. \n","\n","\n","###plan for trial 4\n","\n","For trial 4 I am going to transition to using image data as input. A simple CNN architecture will be used, including convolution, max pooling, and drop out layers. "],"metadata":{"id":"m4AYyLtGJ9f7"}},{"cell_type":"markdown","source":["# Trial 4\n"," \n","reason for change?\n","\n","After experimenting with models taking text as input, I now need to explore image inputs. This will complete my understanding of both inputs and associated models so I can eventually create a multi-modal multi-tasks learner. The model architecture will be composed of multiple convolution blocks, each including a convolution and max-pooling layer. After the convolution blocks the final feature map will be flattened into a 1D vector and fed into the final fully connected layers. There will be 3 convolution blocks followed by a flattening layer and 4 fully connected layer, with drop out between each fully connected layer. The CNN will predict the price range (0,1,2) of the input image associated to a Airbnb listing. \n","\n","expected outcome?\n","\n","This simple CNN architecture should be able to moderately perform on this task. Picking optimal kernel sizes and other such hyper parameters can be very difficult, thus with out hyper parameter tuning this trial most likely will not produce the best results. I would also suspect the text input has more information that could better point to an associated price range, thus the model may perform worse than the text input models seen in previous trials.   "],"metadata":{"id":"a_2Atv0TK8iO"}},{"cell_type":"code","source":["model_cnn = Sequential()\n","# convolution layer with kernel size (3 x 3) using 32 filters\n","# COnvolution layers compute the feature maps form given input\n","# the number of output channels is equal to the number of filters\n","model_cnn.add(Conv2D(32, (3, 3), activation='relu', input_shape= (32,32,3))) \n","# each max pooling layer with (2 x 2) window and stride of 2, \n","# divides the height and width of the input by 2\n","model_cnn.add(MaxPooling2D((2, 2)))\n","model_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n","model_cnn.add(MaxPooling2D((2, 2)))\n","model_cnn.add(Conv2D(64, (3, 3), activation='relu'))\n","model_cnn.add(MaxPooling2D((2, 2)))\n","model_cnn.add(Flatten())\n","model_cnn.add(Dense(128, activation='relu'))\n","model_cnn.add(Dropout(0.5))\n","model_cnn.add(Dense(128, activation='relu'))\n","model_cnn.add(Dropout(0.5))\n","model_cnn.add(Dense(64, activation='relu'))\n","model_cnn.add(Dropout(0.5))\n","model_cnn.add(Dense(3))\n","model_cnn.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0yUtt6Dirylh","executionInfo":{"status":"ok","timestamp":1668632948894,"user_tz":300,"elapsed":19,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"aa618a0c-8cac-4424-a8a6-4c2f96fe4888"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_4\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_10 (Conv2D)          (None, 30, 30, 32)        896       \n","                                                                 \n"," max_pooling2d_9 (MaxPooling  (None, 15, 15, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_11 (Conv2D)          (None, 13, 13, 64)        18496     \n","                                                                 \n"," max_pooling2d_10 (MaxPoolin  (None, 6, 6, 64)         0         \n"," g2D)                                                            \n","                                                                 \n"," conv2d_12 (Conv2D)          (None, 4, 4, 64)          36928     \n","                                                                 \n"," max_pooling2d_11 (MaxPoolin  (None, 2, 2, 64)         0         \n"," g2D)                                                            \n","                                                                 \n"," flatten_2 (Flatten)         (None, 256)               0         \n","                                                                 \n"," dense_21 (Dense)            (None, 128)               32896     \n","                                                                 \n"," dropout_11 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_22 (Dense)            (None, 128)               16512     \n","                                                                 \n"," dropout_12 (Dropout)        (None, 128)               0         \n","                                                                 \n"," dense_23 (Dense)            (None, 64)                8256      \n","                                                                 \n"," dropout_13 (Dropout)        (None, 64)                0         \n","                                                                 \n"," dense_24 (Dense)            (None, 3)                 195       \n","                                                                 \n","=================================================================\n","Total params: 114,179\n","Trainable params: 114,179\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Train the model with the required trianing and validation sets\n","model_cnn.compile(optimizer='adam',\n","              loss='sparse_categorical_crossentropy',\n","              metrics=['SparseCategoricalAccuracy'])\n","\n","# fit model and print the mean of both train and validation accuracies \n","history = model_cnn.fit(x_tr_image, y_tr_price, epochs=20, \n","                    validation_data=(x_vl_image, y_vl_price))\n","print(\"mean val accuracy:\",mean((history.history['val_sparse_categorical_accuracy'])))\n","print(\"mean train accuracy:\",mean((history.history['sparse_categorical_accuracy'])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nru3xheCsnR2","executionInfo":{"status":"ok","timestamp":1668632429832,"user_tz":300,"elapsed":42355,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"9705b218-eaa2-4df4-8f7b-f57db197028b"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","191/191 [==============================] - 6s 8ms/step - loss: 4.8305 - sparse_categorical_accuracy: 0.3768 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 2/20\n","191/191 [==============================] - 1s 5ms/step - loss: 3.4913 - sparse_categorical_accuracy: 0.4303 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 3/20\n","191/191 [==============================] - 1s 5ms/step - loss: 3.1014 - sparse_categorical_accuracy: 0.4816 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 4/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.9320 - sparse_categorical_accuracy: 0.5037 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 5/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.7148 - sparse_categorical_accuracy: 0.4939 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 6/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.3553 - sparse_categorical_accuracy: 0.4448 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 7/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.1494 - sparse_categorical_accuracy: 0.4193 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3047\n","Epoch 8/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.5028 - sparse_categorical_accuracy: 0.3417 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3041\n","Epoch 9/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.2745 - sparse_categorical_accuracy: 0.3570 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3047\n","Epoch 10/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.0433 - sparse_categorical_accuracy: 0.3680 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3047\n","Epoch 11/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.0344 - sparse_categorical_accuracy: 0.3678 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3047\n","Epoch 12/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.0810 - sparse_categorical_accuracy: 0.3578 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3047\n","Epoch 13/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.0760 - sparse_categorical_accuracy: 0.3713 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3047\n","Epoch 14/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.0461 - sparse_categorical_accuracy: 0.3562 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3047\n","Epoch 15/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.0235 - sparse_categorical_accuracy: 0.4434 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 16/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.1900 - sparse_categorical_accuracy: 0.5220 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 17/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.2343 - sparse_categorical_accuracy: 0.5230 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 18/20\n","191/191 [==============================] - 1s 5ms/step - loss: 2.0936 - sparse_categorical_accuracy: 0.5288 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 19/20\n","191/191 [==============================] - 1s 5ms/step - loss: 1.6446 - sparse_categorical_accuracy: 0.3829 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3067\n","Epoch 20/20\n","191/191 [==============================] - 1s 5ms/step - loss: 1.7012 - sparse_categorical_accuracy: 0.3642 - val_loss: 1.0986 - val_sparse_categorical_accuracy: 0.3067\n","mean val accuracy: 0.4726408883929253\n","mean train accuracy: 0.4217177540063858\n"]}]},{"cell_type":"markdown","source":["###thoughts and observations for trial 4\n","\n","Trial 4\n","\n","*   mean validation accuracy - 47.26% \n","*   mean train accuracy       - 42.17% \n","*   test accuracy - 51.67%\n","\n","The results went as expected, there is under fitting present in the train process. The model was not able to accurately predict the price range of the listing based on an associated image. With further tuning of the hyper parameters this accuracy value should be able to increase by a significant amount.\n","\n","###plan for trial 5\n","\n","For trial 5 I will attempt to use transfer learning, specifically using MobileNet, a state of the art CNN trained on Imagenet. The final layers of  MobileNet will not be included, instead new dense layers will be added to learn the images in the dataset given in this assignment. "],"metadata":{"id":"sfFU6WBMO3PW"}},{"cell_type":"markdown","source":["# Trial 5\n"," \n","reason for change?\n","\n","Instead of hyper parameter tuning the model from trial 4, I will attempt to use the MobileNet model which was exhaustively tuned to achieve state of the art performance on the ImageNet dataset. ImageNet is a dataset including a wide range of standard objects that would most defiantly been seen in the dataset given in this assignment. The images from Airbnb listing will include everyday objects that MobileNet trained on ImageNet will most defiantly be able to classify.\n","\n","expected outcome?\n","\n","Although there are many similarities between the ImageNet dataset and the dataset given in this assignment, apply transfer learning may not be able to associate this general objects to the price range of the corresponding image. I suspect the model used for transfer learning will produce better results than the ones seen in trial 4, although without trying there is no way of telling.    "],"metadata":{"id":"OYa5DMIQTfD5"}},{"cell_type":"code","source":["## Loading MobileNet model\n","base_model = MobileNet(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n","base_model.trainable = False ## Not trainable weights\n","# keep the model how it is trained on imagnet, no retraining of these weights"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1bi8IjSE1B5z","executionInfo":{"status":"ok","timestamp":1668633847841,"user_tz":300,"elapsed":3761,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"0173232b-39e7-40eb-c326-9f59882e95d8"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"]},{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet/mobilenet_1_0_224_tf_no_top.h5\n","17225924/17225924 [==============================] - 3s 0us/step\n"]}]},{"cell_type":"code","source":["# Add fully connected layers to be learnt for this specific applicaiton.\n","flatten_layer = Flatten()\n","dense_layer_1 = Dense(50, activation='relu')\n","dropout_1 = Dropout(0.8)\n","dense_layer_2 = Dense(50, activation='relu')\n","prediction_layer = Dense(3, activation='softmax')\n","\n","# Add MobileNet model to the newly created fully connected layers to finish the model\n","model_transfer = Sequential([\n","    base_model,\n","    flatten_layer,\n","    dense_layer_1,\n","    dropout_1,\n","    dense_layer_2,\n","    prediction_layer\n","])"],"metadata":{"id":"rukPQdNE2gV_","executionInfo":{"status":"ok","timestamp":1668633935034,"user_tz":300,"elapsed":3,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Train the model with the required trianing and validation sets\n","model_transfer.compile(loss='sparse_categorical_crossentropy', \n","                       optimizer='adam', metrics=['SparseCategoricalAccuracy']) \n","\n","# fit model and print the mean of both train and validation accuracies \n","history = model_transfer.fit(x_tr_image, y_tr_price, epochs=20, batch_size=batch_size,\n","                   validation_data=(x_vl_image, y_vl_price)) \n","print(\"mean val accuracy:\",mean((history.history['val_sparse_categorical_accuracy'])))\n","print(\"mean train accuracy:\",mean((history.history['sparse_categorical_accuracy'])))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CJvvG2nH3T8a","executionInfo":{"status":"ok","timestamp":1668634162458,"user_tz":300,"elapsed":54797,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"b18ca4b6-be9d-4eaf-bf35-219ab75db8cd"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","382/382 [==============================] - 5s 9ms/step - loss: 0.8416 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8126 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 2/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8407 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.8134 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 3/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8404 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.8102 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 4/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8407 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8093 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 5/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8409 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8101 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 6/20\n","382/382 [==============================] - 2s 6ms/step - loss: 0.8406 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8107 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 7/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8401 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8116 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 8/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8402 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8152 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 9/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8406 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8100 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 10/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8408 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8099 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 11/20\n","382/382 [==============================] - 3s 8ms/step - loss: 0.8399 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8102 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 12/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8400 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8154 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 13/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8396 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8112 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 14/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8409 - sparse_categorical_accuracy: 0.6161 - val_loss: 0.8122 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 15/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8392 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8101 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 16/20\n","382/382 [==============================] - 2s 6ms/step - loss: 0.8400 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8117 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 17/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8397 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8111 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 18/20\n","382/382 [==============================] - 2s 6ms/step - loss: 0.8386 - sparse_categorical_accuracy: 0.6160 - val_loss: 0.8095 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 19/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8410 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8137 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 20/20\n","382/382 [==============================] - 3s 7ms/step - loss: 0.8394 - sparse_categorical_accuracy: 0.6161 - val_loss: 0.8135 - val_sparse_categorical_accuracy: 0.6402\n","mean val accuracy: 0.6402359008789062\n","mean train accuracy: 0.6162760257720947\n"]}]},{"cell_type":"markdown","source":["###thoughts and observations for trial 5\n","Trial 4\n","\n","*   mean validation accuracy - 47.26% \n","*   mean train accuracy       - 42.17% \n","*   test accuracy - 51.67%\n","\n","Trial 5\n","\n","*   mean validation accuracy - 64.02% \n","*   mean train accuracy       - 61.63% \n","*   test accuracy - 62.74%\n","\n","The results went as expected, The transfer learning was able to outperform the simple CNN proposed in trail 4. The transfer learning model performed more than 10% better on the given dataset in this assignment. There is still under fitting present although, image detection tasks are generally harder than other tasks. Performing 70%+ accuracy on a task like this would be considered a success. The image task is not quite there but it is defiantly improving. Further tuning could be conducted to achieve higher accuracy. \n","\n","###plan for trial 6\n","\n","Now that the image and text data are well understood and models have been made to accurately predict the listing price category of given listing I will now combine both inputs into a multi-modality model. The model will take both text and image as input, train on each and concatenate their outputs to get one final output of a predicted listing price category (0,1,2)."],"metadata":{"id":"pez1jqdzWRgu"}},{"cell_type":"markdown","source":["# Trial 6\n"," \n","reason for change?\n","\n","Using multi-modalities to train a model adds an increased amount of information to learn on. The information included in the images will have interconnections to the text data and vice versa. The two may also provide information that the other is missing, for example, the text may describe a large backyard although the image may only include the kitchen. The multi-modal model will output the predicted price category associated with each Airbnb listing (0,1,2).\n","\n","expected outcome?\n","\n","For the reasons just discussed I believe the multi-modal model will be able to outperform previous trails. I suspect the test and validation accuracies to increase. There is also a possibility that the overfitting is decreased due to the lack of overfitting seen in the CNN training. "],"metadata":{"id":"-HZRn-nIZaLC"}},{"cell_type":"code","source":["# Add same attention layer as seen before , this cell is only inclued here \n","# for reference and ease of understanding\n","# refer to previous cell to get comments and source\n","\n","class attention(Layer):\n","    def __init__(self,**kwargs):\n","        super(attention,self).__init__(**kwargs)\n","\n","    def build(self,input_shape):\n","        self.W=self.add_weight(name=\"att_weight\",shape=(input_shape[-1],1),initializer=\"normal\")\n","        self.b=self.add_weight(name=\"att_bias\",shape=(input_shape[1],1),initializer=\"zeros\")        \n","        super(attention, self).build(input_shape)\n","\n","    def call(self,x):\n","        et=K.squeeze(K.tanh(K.dot(x,self.W)+self.b),axis=-1)\n","        at=K.softmax(et)\n","        at=K.expand_dims(at,axis=-1)\n","        output=x*at\n","        return K.sum(output,axis=1)\n","\n","    def compute_output_shape(self,input_shape):\n","        return (input_shape[0],input_shape[-1])\n","\n","    def get_config(self):\n","        return super(attention,self).get_config()"],"metadata":{"id":"FtllnjxP6B3l","executionInfo":{"status":"ok","timestamp":1668635643165,"user_tz":300,"elapsed":340,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# define two sets of inputs\n","input_text = Input(batch_shape=(None, max_len))\n","input_image = Input(batch_shape=(None, 32, 32, 3))\n","\n","num_words = 40000 # tokenizer.num_words\n","maxlen = max_len\n","batch_size = 16\n","\n","# the first branch operates on the first input\n","# same arcitecture of previous trials with text input\n","x_words = (Embedding(num_words, batch_size, input_length=maxlen))(input_text)\n","x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n","x = attention()(x_words)\n","x = Dropout(0.2)(x)\n","x = Dense(50, activation='relu')(x)\n","x = Dropout(0.2)(x)\n","x = Dense(3, activation='softmax')(x)\n","x = Model(inputs=input_text, outputs=x)\n","# the second branch opreates on the second input\n","# same arcitecture of previous trials with image input\n","y = Conv2D(32,(3,3), activation='relu')(input_image)\n","y = MaxPooling2D()(y)\n","y = Conv2D(32,(3,3), activation='relu')(y)\n","y = MaxPooling2D()(y)\n","y = Conv2D(32,(3,3), activation='relu')(y)\n","y = Flatten()(y)\n","y = Dropout(0.5)(y)\n","y = Dense(3, activation='softmax')(y)\n","# model building by supplying inputs/outputs\n","y = Model(inputs=input_image, outputs=y)\n","\n","# combine the output of the two branches\n","combined = tf.concat([x.output, y.output], axis=-1)\n","# apply a FC layer and then a regression prediction on the\n","# combined outputs\n","z = Dense(3, activation=\"softmax\")(combined)\n","# our model will accept the inputs of the two branches and\n","# then output a single value\n","model_multiModal = Model(inputs=[x.input, y.input], outputs=z)\n","\n","model_multiModal.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['SparseCategoricalAccuracy']) \n","\n","model_multiModal.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VdWOAa4p3aPZ","executionInfo":{"status":"ok","timestamp":1668635644220,"user_tz":300,"elapsed":1059,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"95ce0f2f-3d06-472a-e3a1-4f01653d88b6"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_5 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," input_4 (InputLayer)           [(None, 100)]        0           []                               \n","                                                                                                  \n"," conv2d_13 (Conv2D)             (None, 30, 30, 32)   896         ['input_5[0][0]']                \n","                                                                                                  \n"," embedding_3 (Embedding)        (None, 100, 16)      640000      ['input_4[0][0]']                \n","                                                                                                  \n"," max_pooling2d_12 (MaxPooling2D  (None, 15, 15, 32)  0           ['conv2d_13[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," bidirectional_3 (Bidirectional  (None, 100, 100)    26800       ['embedding_3[0][0]']            \n"," )                                                                                                \n","                                                                                                  \n"," conv2d_14 (Conv2D)             (None, 13, 13, 32)   9248        ['max_pooling2d_12[0][0]']       \n","                                                                                                  \n"," attention_2 (attention)        (None, 100)          200         ['bidirectional_3[0][0]']        \n","                                                                                                  \n"," max_pooling2d_13 (MaxPooling2D  (None, 6, 6, 32)    0           ['conv2d_14[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," dropout_18 (Dropout)           (None, 100)          0           ['attention_2[0][0]']            \n","                                                                                                  \n"," conv2d_15 (Conv2D)             (None, 4, 4, 32)     9248        ['max_pooling2d_13[0][0]']       \n","                                                                                                  \n"," dense_34 (Dense)               (None, 50)           5050        ['dropout_18[0][0]']             \n","                                                                                                  \n"," flatten_6 (Flatten)            (None, 512)          0           ['conv2d_15[0][0]']              \n","                                                                                                  \n"," dropout_19 (Dropout)           (None, 50)           0           ['dense_34[0][0]']               \n","                                                                                                  \n"," dropout_20 (Dropout)           (None, 512)          0           ['flatten_6[0][0]']              \n","                                                                                                  \n"," dense_35 (Dense)               (None, 3)            153         ['dropout_19[0][0]']             \n","                                                                                                  \n"," dense_36 (Dense)               (None, 3)            1539        ['dropout_20[0][0]']             \n","                                                                                                  \n"," tf.concat (TFOpLambda)         (None, 6)            0           ['dense_35[0][0]',               \n","                                                                  'dense_36[0][0]']               \n","                                                                                                  \n"," dense_37 (Dense)               (None, 3)            21          ['tf.concat[0][0]']              \n","                                                                                                  \n","==================================================================================================\n","Total params: 693,155\n","Trainable params: 693,155\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Train the model with the required trianing and validation sets\n","history = model_multiModal.fit(x=[x_tr_text_id, x_tr_image], y=y_tr_price,\n","           batch_size=batch_size,\n","           epochs=20,\n","           validation_data=([x_vl_text_id, x_vl_image], y_vl_price))\n","\n","# fit model and print the mean of both train and validation accuracies \n","print(\"mean val accuracy:\",mean((history.history['val_sparse_categorical_accuracy'])))\n","print(\"mean train accuracy:\",mean((history.history['sparse_categorical_accuracy'])))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qh8Bje2A8V-S","executionInfo":{"status":"ok","timestamp":1668635837818,"user_tz":300,"elapsed":190923,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"d0c30af3-923a-4400-bf37-675ca822bd67"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","382/382 [==============================] - 78s 18ms/step - loss: 0.8590 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8138 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 2/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8428 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8103 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 3/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8095 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 4/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8421 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8098 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 5/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8421 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8099 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 6/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8421 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8093 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 7/20\n","382/382 [==============================] - 6s 17ms/step - loss: 0.8421 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8094 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 8/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8421 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8094 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 9/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8447 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8105 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 10/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8093 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 11/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8092 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 12/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8096 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 13/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8425 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8117 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 14/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8425 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8096 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 15/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8419 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8090 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 16/20\n","382/382 [==============================] - 7s 18ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8101 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 17/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8093 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 18/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.8421 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8095 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 19/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8097 - val_sparse_categorical_accuracy: 0.6402\n","Epoch 20/20\n","382/382 [==============================] - 6s 15ms/step - loss: 0.8422 - sparse_categorical_accuracy: 0.6163 - val_loss: 0.8100 - val_sparse_categorical_accuracy: 0.6402\n","mean val accuracy: 0.6402359008789062\n","mean train accuracy: 0.616292417049408\n"]}]},{"cell_type":"markdown","source":["###thoughts and observations for trial 6\n","\n","*   best CNN model test accuracy - 62.74% (transfer learning)\n","*  best text model test accuracy - 65.35% (LSTM + Attention) \n","\n","Trial 6\n","\n","*   mean validation accuracy - 64.02%\n","*   mean train accuracy       - 61.63% \n","*   test accuracy - 63.93%\n","\n","I suspected the model would perform better than previous trails although the test accuracy was still not better than purely using only text as input to a model. It was better than the CNN model which suggests the Image data may not be suited for this application. Overfitting was no longer present which was probably due to the introduction of the CNN which did not experience overfitting. \n","\n","###plan for trial 7\n","\n","For trail 7 the same multi-modal model will be used, although this time it will use a multi-task classifier to predict both listing price category, and the type of listing. Both these outputs were explained in the problem formulation. \n"],"metadata":{"id":"wZf-VYhMbZEE"}},{"cell_type":"markdown","source":["# Trial 7\n"," \n","reason for change?\n","\n","Training a model for multi-task learning is a intuitively more difficult task, as the model must work twice as hard than in the case of a single output. This model will provide a user more information to work with using only one model. This adds value to the solution as only one model needs to be trained to complete the task of two separate models. The same multi-modal model used in trail 6 will be altered to now output multi-task classification to predict both listing price category, and type of listing.\n","\n","expected outcome?\n","\n","I suspect a small dip in performance from the previous trail as the new trial is adding more complexity to the classification task. "],"metadata":{"id":"1OKfFCSieRvP"}},{"cell_type":"code","source":["# define two sets of inputs\n","input_text = Input(batch_shape=(None, max_len))\n","input_image = Input(batch_shape=(None, 32, 32, 3))\n","\n","num_words = 40000 # tokenizer.num_words\n","maxlen = max_len\n","batch_size = 16\n","\n","# apply same multi-modal model to complete the multi-task model\n","\n","# the first branch operates on the first input\n","x_words = (Embedding(num_words, batch_size, input_length=maxlen))(input_text)\n","x_words = Bidirectional(LSTM(50, return_sequences=True))(x_words)\n","x = attention()(x_words) #Attention(maxlen)(x_words)\n","x = Dropout(0.2)(x)\n","x = Dense(50, activation='relu')(x)\n","x = Dropout(0.2)(x)\n","x = Dense(3, activation='softmax')(x)\n","x = Model(inputs=input_text, outputs=x)\n","# the second branch opreates on the second input\n","#inputs = Input(shape=input_shape)\n","y = Conv2D(32,(3,3), activation='relu')(input_image)\n","y = MaxPooling2D()(y)\n","y = Conv2D(32,(3,3), activation='relu')(y)\n","y = MaxPooling2D()(y)\n","y = Conv2D(32,(3,3), activation='relu')(y)\n","y = Flatten()(y)\n","y = Dropout(0.5)(y)\n","y = Dense(3, activation='softmax')(y)\n","# model building by supplying inputs/outputs\n","y = Model(inputs=input_image, outputs=y)\n","\n","# combine the output of the two branches\n","combined = tf.concat([x.output, y.output], axis=-1)\n","# apply a FC layer and then a regression prediction on the\n","# combined outputs\n","\n","# Create two outputs one for each input, both multi-class classificaiton (non-binary)\n","p_price = Dense(len_price, activation='softmax', name='price')(combined)\n","p_type = Dense(len_type, activation='softmax', name='type')(combined)\n","\n","#complete funcitonal model by supplying inputs and outputs\n","model_multiModel_multiObjective = keras.Model(\n","    inputs={\n","        'text': input_text,\n","        'image': input_image,\n","    },\n","    outputs={\n","        'price': p_price,\n","        'type': p_type,\n","    },\n",")\n","\n","#compile funcitonal model and eaqually weight the loss function to both inputs and outputs\n","model_multiModel_multiObjective.compile(\n","    optimizer=Adam(),\n","    loss={\n","        'price': 'sparse_categorical_crossentropy',\n","        'type': 'sparse_categorical_crossentropy',\n","    },\n","    loss_weights={\n","        'price': 0.5,\n","        'type': 0.5,       \n","    },\n","    metrics={\n","        'price': ['SparseCategoricalAccuracy'],\n","        'type': ['SparseCategoricalAccuracy'],\n","    },\n",")\n","\n","model_multiModel_multiObjective.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k_WCqRzu1kEi","executionInfo":{"status":"ok","timestamp":1668545825929,"user_tz":300,"elapsed":985,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"0d638968-22b9-478b-a097-10bfed1b15b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_24\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_24 (InputLayer)          [(None, 32, 32, 3)]  0           []                               \n","                                                                                                  \n"," input_23 (InputLayer)          [(None, 100)]        0           []                               \n","                                                                                                  \n"," conv2d_79 (Conv2D)             (None, 30, 30, 32)   896         ['input_24[0][0]']               \n","                                                                                                  \n"," embedding_13 (Embedding)       (None, 100, 16)      640000      ['input_23[0][0]']               \n","                                                                                                  \n"," max_pooling2d_59 (MaxPooling2D  (None, 15, 15, 32)  0           ['conv2d_79[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," bidirectional_9 (Bidirectional  (None, 100, 100)    26800       ['embedding_13[0][0]']           \n"," )                                                                                                \n","                                                                                                  \n"," conv2d_80 (Conv2D)             (None, 13, 13, 32)   9248        ['max_pooling2d_59[0][0]']       \n","                                                                                                  \n"," attention_9 (attention)        (None, 100)          200         ['bidirectional_9[0][0]']        \n","                                                                                                  \n"," max_pooling2d_60 (MaxPooling2D  (None, 6, 6, 32)    0           ['conv2d_80[0][0]']              \n"," )                                                                                                \n","                                                                                                  \n"," dropout_38 (Dropout)           (None, 100)          0           ['attention_9[0][0]']            \n","                                                                                                  \n"," conv2d_81 (Conv2D)             (None, 4, 4, 32)     9248        ['max_pooling2d_60[0][0]']       \n","                                                                                                  \n"," dense_61 (Dense)               (None, 50)           5050        ['dropout_38[0][0]']             \n","                                                                                                  \n"," flatten_19 (Flatten)           (None, 512)          0           ['conv2d_81[0][0]']              \n","                                                                                                  \n"," dropout_39 (Dropout)           (None, 50)           0           ['dense_61[0][0]']               \n","                                                                                                  \n"," dropout_40 (Dropout)           (None, 512)          0           ['flatten_19[0][0]']             \n","                                                                                                  \n"," dense_62 (Dense)               (None, 3)            153         ['dropout_39[0][0]']             \n","                                                                                                  \n"," dense_63 (Dense)               (None, 3)            1539        ['dropout_40[0][0]']             \n","                                                                                                  \n"," tf.concat_8 (TFOpLambda)       (None, 6)            0           ['dense_62[0][0]',               \n","                                                                  'dense_63[0][0]']               \n","                                                                                                  \n"," price (Dense)                  (None, 3)            21          ['tf.concat_8[0][0]']            \n","                                                                                                  \n"," type (Dense)                   (None, 24)           168         ['tf.concat_8[0][0]']            \n","                                                                                                  \n","==================================================================================================\n","Total params: 693,323\n","Trainable params: 693,323\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["# Train the model with the required trianing and validation sets\n","# require you to input as dictionary to better map each output and input based on modality\n","history = model_multiModel_multiObjective.fit(\n","    x={\n","        'text': x_tr_text_id,\n","        'image': x_tr_image\n","    },\n","    y={\n","        'price': y_tr_price,\n","        'type': y_tr_type,\n","    },\n","    epochs=20,\n","    batch_size=16,\n","    validation_data=(\n","        {\n","            'text': x_vl_text_id,\n","            'image': x_vl_image,\n","         }, \n","        {\n","            'price': y_vl_price,\n","            'type': y_vl_type,\n","        }))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xv8eHTLhCnki","executionInfo":{"status":"ok","timestamp":1668545974190,"user_tz":300,"elapsed":145887,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"7c53997a-ac2e-4fe3-949c-3691dfbc137d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/20\n","382/382 [==============================] - 11s 19ms/step - loss: 1.7145 - price_loss: 0.8656 - type_loss: 2.5635 - price_sparse_categorical_accuracy: 0.6202 - type_sparse_categorical_accuracy: 0.4868 - val_loss: 1.3762 - val_price_loss: 0.8518 - val_type_loss: 1.9005 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 2/20\n","382/382 [==============================] - 7s 18ms/step - loss: 1.1816 - price_loss: 0.8342 - type_loss: 1.5290 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 1.0630 - val_price_loss: 0.8503 - val_type_loss: 1.2757 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 3/20\n","382/382 [==============================] - 6s 17ms/step - loss: 1.0016 - price_loss: 0.8320 - type_loss: 1.1711 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9794 - val_price_loss: 0.8515 - val_type_loss: 1.1074 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 4/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9557 - price_loss: 0.8321 - type_loss: 1.0794 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9543 - val_price_loss: 0.8516 - val_type_loss: 1.0570 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 5/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9372 - price_loss: 0.8316 - type_loss: 1.0429 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9443 - val_price_loss: 0.8521 - val_type_loss: 1.0366 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 6/20\n","382/382 [==============================] - 6s 17ms/step - loss: 0.9297 - price_loss: 0.8317 - type_loss: 1.0277 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9389 - val_price_loss: 0.8516 - val_type_loss: 1.0261 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 7/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9257 - price_loss: 0.8318 - type_loss: 1.0196 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9362 - val_price_loss: 0.8518 - val_type_loss: 1.0206 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 8/20\n","382/382 [==============================] - 6s 17ms/step - loss: 0.9233 - price_loss: 0.8317 - type_loss: 1.0148 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9343 - val_price_loss: 0.8518 - val_type_loss: 1.0169 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 9/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9218 - price_loss: 0.8318 - type_loss: 1.0118 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9334 - val_price_loss: 0.8521 - val_type_loss: 1.0147 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 10/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9208 - price_loss: 0.8319 - type_loss: 1.0098 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9327 - val_price_loss: 0.8519 - val_type_loss: 1.0134 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 11/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9200 - price_loss: 0.8317 - type_loss: 1.0084 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9322 - val_price_loss: 0.8520 - val_type_loss: 1.0123 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 12/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9196 - price_loss: 0.8319 - type_loss: 1.0074 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9317 - val_price_loss: 0.8516 - val_type_loss: 1.0117 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 13/20\n","382/382 [==============================] - 7s 18ms/step - loss: 0.9193 - price_loss: 0.8318 - type_loss: 1.0068 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9317 - val_price_loss: 0.8519 - val_type_loss: 1.0114 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 14/20\n","382/382 [==============================] - 6s 17ms/step - loss: 0.9190 - price_loss: 0.8317 - type_loss: 1.0062 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9315 - val_price_loss: 0.8518 - val_type_loss: 1.0111 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 15/20\n","382/382 [==============================] - 6s 17ms/step - loss: 0.9189 - price_loss: 0.8317 - type_loss: 1.0060 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9313 - val_price_loss: 0.8517 - val_type_loss: 1.0109 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 16/20\n","382/382 [==============================] - 6s 17ms/step - loss: 0.9186 - price_loss: 0.8317 - type_loss: 1.0055 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9315 - val_price_loss: 0.8521 - val_type_loss: 1.0108 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 17/20\n","382/382 [==============================] - 6s 17ms/step - loss: 0.9187 - price_loss: 0.8319 - type_loss: 1.0054 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9312 - val_price_loss: 0.8517 - val_type_loss: 1.0107 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 18/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9185 - price_loss: 0.8318 - type_loss: 1.0053 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9311 - val_price_loss: 0.8515 - val_type_loss: 1.0107 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 19/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9185 - price_loss: 0.8318 - type_loss: 1.0051 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9311 - val_price_loss: 0.8513 - val_type_loss: 1.0108 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n","Epoch 20/20\n","382/382 [==============================] - 6s 16ms/step - loss: 0.9183 - price_loss: 0.8316 - type_loss: 1.0050 - price_sparse_categorical_accuracy: 0.6204 - type_sparse_categorical_accuracy: 0.7564 - val_loss: 0.9314 - val_price_loss: 0.8519 - val_type_loss: 1.0108 - val_price_sparse_categorical_accuracy: 0.6239 - val_type_sparse_categorical_accuracy: 0.7536\n"]}]},{"cell_type":"markdown","source":["###thoughts and observations for trial 7\n","\n","The model had almost identical results with regard to classifying the price category of the listing. The type category was not training on during previous trails so there is no baseline to compare with. The results came as a surprise as I suspected the multi-task classifier would perform worse on a more complicated task. The final multi-modal multi-task model was able to accurately classify both listing price category and type using image and text data validating the success of the trials. "],"metadata":{"id":"7sz1Y6XOeZev"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZoqKB--AZYgc","outputId":"dfa42760-5908-475b-bf4a-088b59059790","executionInfo":{"status":"ok","timestamp":1668196435848,"user_tz":300,"elapsed":1162,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["230/230 [==============================] - 1s 2ms/step\n","[[1.0000000e+00 1.3702177e-12 7.7522864e-17]\n"," [1.0000000e+00 1.6480656e-17 1.8595134e-24]\n"," [9.9999976e-01 8.1405667e-13 2.6257294e-07]\n"," ...\n"," [9.9391705e-01 6.0828989e-03 6.2661670e-08]\n"," [9.9999356e-01 6.4774149e-06 1.0875747e-17]\n"," [1.0000000e+00 1.0245475e-13 1.0994457e-16]]\n","[0 0 0 ... 0 0 0]\n"]}],"source":["# The following cells were used to create dataframes used for the test set \n","# and submission for kaggle \n","# this cell is used for the multi-task trial \n","y_predict = model.predict(\n","    {\n","        'summary': x_test_summary,\n","        'image': x_test_image\n","    }\n",")\n","\n","price_predicted = y_predict['price']\n","print(price_predicted)\n","price_category_predicted = np.argmax(price_predicted, axis=1)\n","print(price_category_predicted)\n","\n","pd.DataFrame(\n","    {'id': x_test_df.id,\n","     'price': price_category_predicted}).to_csv('sample_submission.csv', index=False)"]},{"cell_type":"code","source":["# The following cells were used to create dataframes used for the test set \n","# and submission for kaggle \n","# this cell is used for all other trials excluding the multi-task trial \n","\n","y_predict = model_text.predict(x_test_summary)\n","\n","price_predicted = y_predict\n","print(price_predicted)\n","price_category_predicted = np.argmax(price_predicted, axis=1)\n","print(price_category_predicted)\n","\n","\n","pd.DataFrame(\n","    {'id': x_test_df.id,\n","     'price': price_category_predicted}).to_csv('LSTM_Simple_Submission.csv', index=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qzeZ_CqgAXn-","executionInfo":{"status":"ok","timestamp":1668197433598,"user_tz":300,"elapsed":2102,"user":{"displayName":"Dusty Pulver","userId":"16342991633441224682"}},"outputId":"dba28f17-559a-4e1a-c6a6-9332a0f7060e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["230/230 [==============================] - 1s 5ms/step\n","[[5.2713811e-02 9.4037485e-01 6.9114123e-03]\n"," [9.9993253e-01 5.7930345e-05 9.4949783e-06]\n"," [9.9997616e-01 2.2194465e-05 1.6442051e-06]\n"," ...\n"," [4.9694651e-01 4.6446735e-01 3.8586162e-02]\n"," [9.9998868e-01 1.0360092e-05 9.5379721e-07]\n"," [9.4733905e-04 9.9161679e-01 7.4358205e-03]]\n","[1 0 0 ... 0 0 1]\n"]}]},{"cell_type":"markdown","source":["🌈Is fully-connected model a good one for sequential data? Why? How about for image data? Is it good? Why?\n","\n","Simple fully-connected models are not good for sequential data as they do not have any ability to retain any information in memory. The ideal models for sequential data are RNNs, more specifically an LSTM. Which can retian long-short term memory to help give context to points in a data sequence. \n","\n","Simple fully-connected models are not good for image data as they can not extract features as easily or effectient compare to alterantives. With images as input a simple fully-connected model would need a node for every color channel of every pixel. CNNs are much more efficient and extract features from an image in a better fasion. \n","\n","🌈What is gradient vanishing and gradient explosion, and how GRU/LSTM tries to mitigate this problem?\n","\n","The vanishing gradient problem, states that with more layers added to a model, the gradients of the loss fucntions increasingly approach 0. This results in a gradient descent that never converges to the optimum. \n","\n","The exploding gradient problem is one where the gradients get exponentially large casuing very large weight updtes. These large weight updates case the gradient descent to diverge.\n","\n","GRU/LSTM use the gates in their arcitecture to regulate the values in the gradient decsent by updating its parameters. Specifically the forget gate can select whether to keep or forget informaiton allowing to better regualte the passing gradient. \n","\n","🌈What is multi-objective/multi-task learning? What is multi-modality learning? How do you use them in this assignment?\n","\n","In multi-objective/multi-task learning there are multiple target values your model is trying to solve for. The bias between the two tasks within the model is shared forcing the model to allow for trade offs between the different tasks. Multi-modality learning is the use of multiple types of data as input to your model. Each data source needs to be processed individually as they will have different characteristics. In this assignment I used multi-modality learning with image and text data as input to a model to predict listing price range. I also used multi-objective/multi-task learning to learn both the listing price range and the type of listing. \n","\n","🌈What is the difference among xgboost, lightgbm and catboost? \n","\n","All 3 of these are gradient boosting algorithms. The main difference between the algorithms is the splitting condition, CatBoost uses symmetric trees and the other two use asymmetric trees. This refers to the splitting condition being consistent across all nodes at the same depth of the tree. Catboost is also designed for categorical data and performs better than the other two on such data.\n","\n"],"metadata":{"id":"lwoyKK2NFIb9"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"95f807319dad4c5d8a735ff28c2aa33f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0432bf5b52cb449e97a15eab0069264b","IPY_MODEL_34f87b3cf05345a9ae3ad45400d2b4f8","IPY_MODEL_ee2d6274b97e40a186fe1f1c2a63f188"],"layout":"IPY_MODEL_1078c1420028485f8b216b2d6a802097"}},"0432bf5b52cb449e97a15eab0069264b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5f2a7c72a824b4d9788ae62265693b7","placeholder":"​","style":"IPY_MODEL_7e917e0737234a268bfb900790c923b7","value":"100%"}},"34f87b3cf05345a9ae3ad45400d2b4f8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_58825daa17cf4c56bbe32f83b245d4fd","max":7627,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35daeee4dc744cf4991a06346d55e2d1","value":7627}},"ee2d6274b97e40a186fe1f1c2a63f188":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_331cdebf857a4df8a209a9643a9345c9","placeholder":"​","style":"IPY_MODEL_454400d769b941189c6c8ec4af9141fb","value":" 7627/7627 [00:59&lt;00:00, 129.66it/s]"}},"1078c1420028485f8b216b2d6a802097":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5f2a7c72a824b4d9788ae62265693b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e917e0737234a268bfb900790c923b7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58825daa17cf4c56bbe32f83b245d4fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35daeee4dc744cf4991a06346d55e2d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"331cdebf857a4df8a209a9643a9345c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"454400d769b941189c6c8ec4af9141fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e20b568fd87f4894bc1959ef754205cf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_78760a297e6a42c196145d9bdb7297e8","IPY_MODEL_b4a12b96c3d8411d9774376e9c9e9c5e","IPY_MODEL_1bc753cbe8c64121ab7f19443f5178f8"],"layout":"IPY_MODEL_18deb17ab1de42c0a265b376830c6e42"}},"78760a297e6a42c196145d9bdb7297e8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a069732fcd774c9096e0f7bf0215e531","placeholder":"​","style":"IPY_MODEL_264e4635ad0b4136a859d8488f276962","value":"100%"}},"b4a12b96c3d8411d9774376e9c9e9c5e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4659dd22265841b3a852e93dbc590430","max":7360,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0824af7993d4f548379ce9d63f61e86","value":7360}},"1bc753cbe8c64121ab7f19443f5178f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30d11a9cfff043c5ac3b2a5fb6d08982","placeholder":"​","style":"IPY_MODEL_b2be3c79a45049a796c91dc32f415c8e","value":" 7360/7360 [00:57&lt;00:00, 130.93it/s]"}},"18deb17ab1de42c0a265b376830c6e42":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a069732fcd774c9096e0f7bf0215e531":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"264e4635ad0b4136a859d8488f276962":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4659dd22265841b3a852e93dbc590430":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0824af7993d4f548379ce9d63f61e86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"30d11a9cfff043c5ac3b2a5fb6d08982":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b2be3c79a45049a796c91dc32f415c8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}